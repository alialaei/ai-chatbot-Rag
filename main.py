from __future__ import annotations

import os
from pathlib import Path

from fastapi import FastAPI, File, HTTPException, UploadFile, status
from pydantic import BaseModel, Field
from dotenv import load_dotenv

from rag_engine import IngestionStatus, load_and_store_pdf, search_and_answer

# ---------------------------------------------------------------------------
# Environment & constants
# ---------------------------------------------------------------------------

load_dotenv()
TEMP_DIR = Path("temp")
TEMP_DIR.mkdir(exist_ok=True)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError(
        "OPENAI_API_KEY missing – set it in .env or environment variables."  # noqa: E501
    )

# ---------------------------------------------------------------------------
# FastAPI application
# ---------------------------------------------------------------------------

app = FastAPI(title="AI Chatbot with RAG & Qdrant", version="0.1.0")

# ----------------------------- Models --------------------------------------

class ChatRequest(BaseModel):
    """Payload for /chat."""

    message: str = Field(..., min_length=3)
    client_id: str = Field(..., min_length=1, description="Tenant identifier")

# ----------------------------- Routes --------------------------------------

@app.post("/upload-docs", response_model=IngestionStatus)
async def upload_docs(client_id: str, file: UploadFile = File(...)) -> IngestionStatus:  # type: ignore[valid-type]
    """Upload a PDF document and store embeddings for *client_id*."""

    if not file.filename.lower().endswith(".pdf"):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Only PDF files are supported for now.",
        )

    file_path = TEMP_DIR / file.filename
    file_path.write_bytes(await file.read())  # synchronous write is fine for small files

    return load_and_store_pdf(file_path, client_id)


@app.post("/chat")
async def chat_endpoint(chat: ChatRequest) -> dict[str, str]:
    """Return an answer generated by GPT‑4 using RAG for *client_id*."""

    try:
        answer: str = search_and_answer(chat.message, chat.client_id)
    except Exception as exc:  # broad catch for demo; narrow in production
        raise HTTPException(status_code=500, detail=str(exc)) from exc

    return {"response": answer}